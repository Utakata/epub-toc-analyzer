#!/usr/bin/env python
# -*- coding: utf-8 -*-

__license__   = 'GPL v3'
__copyright__ = '2025, Enhanced Word TOC Output for Japan v2.0'
__docformat__ = 'restructuredtext en'
__version__ = '2.0.0'

import sys, re, os, traceback, copy, glob
from posixpath import normpath
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import chardet

# Progress bar support
try:
    from tqdm import tqdm
    PROGRESS_SUPPORT = True
except ImportError:
    PROGRESS_SUPPORT = False
    
logger = logging.getLogger(__name__)

from zipfile import ZipFile, ZIP_STORED, ZIP_DEFLATED
from xml.dom.minidom import parse, parseString, getDOMImplementation, Element
from time import time
from datetime import datetime

import six
from six.moves.urllib.parse import unquote
from six import string_types, text_type as unicode
from six import unichr

from bs4 import BeautifulSoup

# Word document generation support
try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    WORD_SUPPORT = True
except ImportError:
    WORD_SUPPORT = False
    print("python-docx not available. Word output will be disabled.")

class CalibreCompatibleTOCDetector:
    """
    Calibreäº’æ›ã®ç›®æ¬¡æ¤œå‡ºã‚¯ãƒ©ã‚¹
    XPathå¼ãƒ™ãƒ¼ã‚¹ã®éšå±¤æ¤œå‡ºã¨ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè£…
    """
    
    def __init__(self):
        # Calibreé¢¨XPathå¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.xpath_patterns = {
            'level1': ['//h:h1', '//h1', '//div[@class="chapter"]', '//div[@class="section1"]'],
            'level2': ['//h:h2', '//h2', '//div[@class="section"]', '//div[@class="section2"]'],
            'level3': ['//h:h3', '//h3', '//div[@class="subsection"]', '//div[@class="section3"]']
        }
        
        # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.heuristic_patterns = {
            'chapter': re.compile(r'(ç¬¬\d+ç« |Chapter\s+\d+|CHAPTER\s+\d+)', re.IGNORECASE),
            'section': re.compile(r'(\d+\.\d+|\d+ï¼\d+|Â§\d+)', re.IGNORECASE),
            'subsection': re.compile(r'(\d+\.\d+\.\d+|\(\d+\))', re.IGNORECASE)
        }
    
    def detect_toc_from_html(self, html_content):
        """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ç›®æ¬¡æ§‹é€ ã‚’æ¤œå‡º"""
        soup = BeautifulSoup(html_content, 'lxml')
        detected_toc = {'level1': [], 'level2': [], 'level3': []}
        
        # XPathå¼ã«ç›¸å½“ã™ã‚‹CSS Selectorã§æ¤œå‡º
        for level, patterns in [('level1', ['h1', 'div.chapter', 'div.section1']),
                               ('level2', ['h2', 'div.section', 'div.section2']),
                               ('level3', ['h3', 'div.subsection', 'div.section3'])]:
            
            for pattern in patterns:
                elements = soup.select(pattern)
                for elem in elements:
                    text = elem.get_text().strip()
                    if text and len(text) > 1:
                        detected_toc[level].append({
                            'text': self._clean_heading_text(text),
                            'tag': elem.name,
                            'position': self._get_element_position(elem)
                        })
        
        # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æ¤œå‡º
        self._apply_heuristic_detection(soup, detected_toc)
        
        return detected_toc
    
    def _clean_heading_text(self, text):
        """è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # ä¸è¦ãªæ–‡å­—ã®é™¤å»
        text = re.sub(r'\s+', ' ', text).strip()
        # æ—¥æœ¬èªç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ­£è¦åŒ–
        text = re.sub(r'^ç¬¬(\d+)ç« \s*', r'ç¬¬\1ç« ã€€', text)
        text = re.sub(r'^(\d+)\.\s*', r'\1. ', text)
        return text
    
    def _get_element_position(self, element):
        """è¦ç´ ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ä½ç½®ã‚’å–å¾—"""
        # BeautifulSoupã§ã®æ¦‚ç®—ä½ç½®
        return len(str(element.encode_contents()))
    
    def _apply_heuristic_detection(self, soup, detected_toc):
        """Calibreé¢¨ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æ¤œå‡º"""
        all_paragraphs = soup.find_all(['p', 'div', 'span'])
        
        for para in all_paragraphs:
            text = para.get_text().strip()
            if not text:
                continue
                
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            if self.heuristic_patterns['chapter'].search(text):
                if len(text) < 100:  # é•·ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                    detected_toc['level1'].append({
                        'text': self._clean_heading_text(text),
                        'tag': 'heuristic_h1',
                        'position': self._get_element_position(para)
                    })
            elif self.heuristic_patterns['section'].search(text):
                if len(text) < 80:
                    detected_toc['level2'].append({
                        'text': self._clean_heading_text(text),
                        'tag': 'heuristic_h2',
                        'position': self._get_element_position(para)
                    })
            elif self.heuristic_patterns['subsection'].search(text):
                if len(text) < 60:
                    detected_toc['level3'].append({
                        'text': self._clean_heading_text(text),
                        'tag': 'heuristic_h3',
                        'position': self._get_element_position(para)
                    })

class BatchProcessor:
    """ãƒãƒƒãƒå‡¦ç†ã‚¯ãƒ©ã‚¹ - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†"""
    
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.results = []
        self.errors = []
    
    def process_directory(self, directory_path, output_dir=".", format_type="both", recursive=True):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®EPUBãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†"""
        epub_files = self._find_epub_files(directory_path, recursive)
        
        if not epub_files:
            print(f"ğŸ“ {directory_path} ã«EPUBãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []
        
        print(f"ğŸ“š {len(epub_files)}å€‹ã®EPUBãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ããƒãƒƒãƒå‡¦ç†
        if PROGRESS_SUPPORT:
            with tqdm(total=len(epub_files), desc="EPUBå‡¦ç†ä¸­") as pbar:
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = []
                    for epub_file in epub_files:
                        future = executor.submit(self._process_single_epub, epub_file, output_dir, format_type)
                        futures.append(future)
                    
                    for future in futures:
                        try:
                            result = future.result()
                            self.results.append(result)
                            pbar.update(1)
                        except Exception as e:
                            self.errors.append(str(e))
                            pbar.update(1)
        else:
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãªã—ã®å‡¦ç†
            for i, epub_file in enumerate(epub_files, 1):
                print(f"å‡¦ç†ä¸­... {i}/{len(epub_files)}: {Path(epub_file).name}")
                try:
                    result = self._process_single_epub(epub_file, output_dir, format_type)
                    self.results.append(result)
                except Exception as e:
                    self.errors.append(str(e))
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return self.results
    
    def _find_epub_files(self, directory_path, recursive=True):
        """EPUBãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        directory = Path(directory_path)
        if not directory.exists():
            return []
        
        if recursive:
            return list(directory.rglob("*.epub"))
        else:
            return list(directory.glob("*.epub"))
    
    def _process_single_epub(self, epub_path, output_dir, format_type):
        """å˜ä¸€EPUBãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
        try:
            with open(epub_path, 'rb') as f:
                epub_splitter = SplitEpubWordTOC(f)
                results = epub_splitter.generate_word_toc_output(
                    output_dir=output_dir,
                    format_type=format_type
                )
            return {'file': str(epub_path), 'results': results, 'status': 'success'}
        except Exception as e:
            return {'file': str(epub_path), 'error': str(e), 'status': 'error'}

class EnhancedWordTOCGenerator:
    """æ”¹è‰¯ç‰ˆWordå½¢å¼ã®ç›®æ¬¡ãƒ¬ãƒ™ãƒ«3æ®µéšå‡ºåŠ›ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.levels = {1: [], 2: [], 3: []}
        self.current_level_1 = None
        self.current_level_2 = None
        self.book_title = ""
        self.authors = []
        self.calibre_detector = CalibreCompatibleTOCDetector()
        
    def add_toc_entry(self, text, level, href="", anchor="", hierarchy_path="", detected_method="standard"):
        """ç›®æ¬¡ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ï¼ˆæ¤œå‡ºæ–¹æ³•ã®è¨˜éŒ²ä»˜ãï¼‰"""
        entry = {
            'text': text,
            'href': href,
            'anchor': anchor,
            'hierarchy_path': hierarchy_path,
            'level': level,
            'detected_method': detected_method,  # æ¤œå‡ºæ–¹æ³•ã‚’è¨˜éŒ²
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        if level == 1:
            self.current_level_1 = text
            self.current_level_2 = None
            self.levels[1].append(entry)
        elif level == 2:
            self.current_level_2 = text
            entry['parent_level_1'] = self.current_level_1
            self.levels[2].append(entry)
        elif level == 3:
            entry['parent_level_1'] = self.current_level_1
            entry['parent_level_2'] = self.current_level_2
            self.levels[3].append(entry)
    
    def generate_enhanced_text_output(self):
        """æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®ç›®æ¬¡å‡ºåŠ›"""
        output = []
        output.append("=" * 80)
        output.append(f"ğŸ“š æ›¸ç±ç›®æ¬¡æ§‹é€ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ v2.0 (Calibreäº’æ›)")
        output.append(f"æ›¸ç±å: {self.book_title}")
        output.append(f"è‘—è€…: {', '.join(self.authors)}")
        output.append(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        output.append("=" * 80)
        output.append("")
        
        # æ¤œå‡ºçµ±è¨ˆ
        total_entries = sum(len(self.levels[i]) for i in range(1, 4))
        output.append("ğŸ” æ¤œå‡ºçµ±è¨ˆ")
        output.append("-" * 60)
        output.append(f"ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {total_entries}")
        
        # æ¤œå‡ºæ–¹æ³•åˆ¥ã®çµ±è¨ˆ
        detection_stats = {}
        for level in range(1, 4):
            for entry in self.levels[level]:
                method = entry.get('detected_method', 'unknown')
                detection_stats[method] = detection_stats.get(method, 0) + 1
        
        for method, count in detection_stats.items():
            output.append(f"  - {method}: {count}ä»¶")
        output.append("")
        
        # å„ãƒ¬ãƒ™ãƒ«ã®è©³ç´°
        for level in range(1, 4):
            if not self.levels[level]:
                continue
                
            level_names = {1: "ğŸ“– ç›®æ¬¡ãƒ¬ãƒ™ãƒ«1ï¼ˆå¤§è¦‹å‡ºã—ï¼‰", 
                          2: "ğŸ“ ç›®æ¬¡ãƒ¬ãƒ™ãƒ«2ï¼ˆä¸­è¦‹å‡ºã—ï¼‰", 
                          3: "ğŸ“„ ç›®æ¬¡ãƒ¬ãƒ™ãƒ«3ï¼ˆå°è¦‹å‡ºã—ï¼‰"}
            
            output.append(level_names[level])
            output.append("-" * 60)
            
            for i, entry in enumerate(self.levels[level], 1):
                output.append(f"{i:2d}. {entry['text']}")
                
                # è¦ªéšå±¤ã®è¡¨ç¤º
                if level >= 2 and entry.get('parent_level_1'):
                    output.append(f"     â”” è¦ªãƒ¬ãƒ™ãƒ«1: {entry['parent_level_1']}")
                if level == 3 and entry.get('parent_level_2'):
                    output.append(f"       â”” è¦ªãƒ¬ãƒ™ãƒ«2: {entry['parent_level_2']}")
                
                # æŠ€è¡“çš„è©³ç´°
                if entry['href']:
                    output.append(f"     ãƒ•ã‚¡ã‚¤ãƒ«: {entry['href']}")
                if entry['anchor']:
                    output.append(f"     ã‚¢ãƒ³ã‚«ãƒ¼: #{entry['anchor']}")
                if entry.get('detected_method') != 'standard':
                    output.append(f"     æ¤œå‡ºæ–¹æ³•: {entry['detected_method']}")
            output.append("")
        
        return '\n'.join(output)
    
    def generate_enhanced_word_document(self, output_path):
        """æ”¹è‰¯ç‰ˆWordæ–‡æ›¸ã¨ã—ã¦ç›®æ¬¡ã‚’å‡ºåŠ›"""
        if not WORD_SUPPORT:
            raise ImportError("python-docx is required for Word output")
            
        doc = Document()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«
        title = doc.add_heading('æ›¸ç±ç›®æ¬¡æ§‹é€ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ v2.0', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ›¸ç±æƒ…å ±
        info_para = doc.add_paragraph()
        info_para.add_run('æ›¸ç±å: ').bold = True
        info_para.add_run(self.book_title)
        info_para.add_run('\nè‘—è€…: ').bold = True
        info_para.add_run(', '.join(self.authors))
        info_para.add_run('\nç”Ÿæˆæ—¥æ™‚: ').bold = True
        info_para.add_run(datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S'))
        info_para.add_run('\nè§£æã‚¨ãƒ³ã‚¸ãƒ³: ').bold = True
        info_para.add_run('Calibreäº’æ› + ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æ¤œå‡º')
        
        # çµ±è¨ˆæƒ…å ±ã‚’å…ˆé ­ã«
        stats_heading = doc.add_heading('ğŸ“Š æ¤œå‡ºçµ±è¨ˆ', level=1)
        stats_para = doc.add_paragraph()
        stats_para.add_run('ãƒ¬ãƒ™ãƒ«1ã‚¨ãƒ³ãƒˆãƒªæ•°: ').bold = True
        stats_para.add_run(str(len(self.levels[1])))
        stats_para.add_run('\nãƒ¬ãƒ™ãƒ«2ã‚¨ãƒ³ãƒˆãƒªæ•°: ').bold = True
        stats_para.add_run(str(len(self.levels[2])))
        stats_para.add_run('\nãƒ¬ãƒ™ãƒ«3ã‚¨ãƒ³ãƒˆãƒªæ•°: ').bold = True
        stats_para.add_run(str(len(self.levels[3])))
        
        doc.add_page_break()
        
        # å„ãƒ¬ãƒ™ãƒ«ã®ç›®æ¬¡
        level_names = {1: 'ğŸ“– ç›®æ¬¡ãƒ¬ãƒ™ãƒ«1ï¼ˆå¤§è¦‹å‡ºã—ï¼‰', 
                      2: 'ğŸ“ ç›®æ¬¡ãƒ¬ãƒ™ãƒ«2ï¼ˆä¸­è¦‹å‡ºã—ï¼‰', 
                      3: 'ğŸ“„ ç›®æ¬¡ãƒ¬ãƒ™ãƒ«3ï¼ˆå°è¦‹å‡ºã—ï¼‰'}
        
        for level in range(1, 4):
            if not self.levels[level]:
                continue
                
            level_heading = doc.add_heading(level_names[level], level=1)
            
            for i, entry in enumerate(self.levels[level], 1):
                para = doc.add_paragraph(style='List Number')
                para.add_run(entry['text']).bold = True
                
                # éšå±¤æƒ…å ±
                if level >= 2:
                    parent_para = doc.add_paragraph(style='List Bullet 2')
                    if entry.get('parent_level_1'):
                        parent_para.add_run(f"â”” è¦ªãƒ¬ãƒ™ãƒ«1: {entry['parent_level_1']}")
                    if level == 3 and entry.get('parent_level_2'):
                        parent_para.add_run(f"\n  â”” è¦ªãƒ¬ãƒ™ãƒ«2: {entry['parent_level_2']}")
                
                # æŠ€è¡“çš„è©³ç´°
                if entry['href'] or entry['anchor'] or entry.get('detected_method') != 'standard':
                    detail_para = doc.add_paragraph(style='List Bullet 3')
                    details = []
                    if entry['href']:
                        details.append(f"ãƒ•ã‚¡ã‚¤ãƒ«: {entry['href']}")
                    if entry['anchor']:
                        details.append(f"ã‚¢ãƒ³ã‚«ãƒ¼: #{entry['anchor']}")
                    if entry.get('detected_method') != 'standard':
                        details.append(f"æ¤œå‡º: {entry['detected_method']}")
                    detail_para.add_run("   ".join(details))
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¿å­˜
        doc.save(output_path)
        return output_path

# ãƒ¡ã‚¤ãƒ³ã®SplitEpubWordTOCã‚¯ãƒ©ã‚¹ã‚‚æ›´æ–°
class SplitEpubWordTOC:
    """EPUB splitter with enhanced Word TOC output support v2.0"""
    
    def __init__(self, inputio):
        self.epub = ZipFile(inputio, 'r')
        self.content_dom = None
        self.content_relpath = None
        self.manifest_items = None
        self.guide_items = None
        self.toc_dom = None
        self.toc_relpath = None
        self.toc_map = None
        self.split_lines = None
        self.origauthors = []
        self.origtitle = None
        
        # Enhanced features v2.0
        self.toc_processor = None
        self.hierarchy_enabled = True
        self.calibre_detector = CalibreCompatibleTOCDetector()
        
    # ... (ä»¥å‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯åŒã˜ãªã®ã§çœç•¥) ...
    
    def generate_word_toc_output(self, output_dir=".", format_type="both"):
        """
        v2.0æ”¹è‰¯ç‰ˆ: Wordå½¢å¼ç›®æ¬¡ãƒ¬ãƒ™ãƒ«3æ®µéšå‡ºåŠ›ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
        Calibreäº’æ›ã®æ¤œå‡ºæ©Ÿèƒ½ã¨ã‚¨ãƒ©ãƒ¼å‡¦ç†å¼·åŒ–
        """
        try:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã¨å‡¦ç†
            self._detect_and_handle_encoding()
            
            # TOCè§£æå®Ÿè¡Œ
            self.get_enhanced_toc_map()
            
            if not self.toc_processor:
                raise Exception("TOC processor not initialized")
            
            results = {}
            base_name = self._sanitize_filename(self.get_title())
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªã¨ä½œæˆ
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼å‡ºåŠ›
            if format_type in ["text", "both"]:
                text_output = self.toc_processor.word_toc_generator.generate_enhanced_text_output()
                text_path = output_path / f"{base_name}_ç›®æ¬¡åˆ†æ_{timestamp}.txt"
                
                with open(text_path, 'w', encoding='utf-8') as f:
                    f.write(text_output)
                
                results['text'] = str(text_path)
                print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ç›®æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {text_path}")
            
            # Wordæ–‡æ›¸å‡ºåŠ›
            if format_type in ["word", "both"] and WORD_SUPPORT:
                word_path = output_path / f"{base_name}_ç›®æ¬¡åˆ†æ_{timestamp}.docx"
                self.toc_processor.word_toc_generator.generate_enhanced_word_document(str(word_path))
                results['word'] = str(word_path)
                print(f"ğŸ“ Wordæ–‡æ›¸ç›®æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {word_path}")
            elif format_type in ["word", "both"] and not WORD_SUPPORT:
                print("âš ï¸  Wordå‡ºåŠ›ã«ã¯python-docxãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")
            
            return results
            
        except Exception as e:
            logger.error(f"TOCç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise Exception(f"ç›®æ¬¡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    def _detect_and_handle_encoding(self):
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã¨å‡¦ç†"""
        try:
            # EPUBãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            container = self.epub.read("META-INF/container.xml")
            if isinstance(container, bytes):
                detected = chardet.detect(container)
                if detected['confidence'] > 0.7:
                    logger.info(f"Detected encoding: {detected['encoding']}")
        except:
            pass  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
    
    def _sanitize_filename(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã®ç„¡åŠ¹æ–‡å­—ã‚’é™¤å»"""
        if not filename:
            return "unknown_book"
        # Windows/Linuxä¸¡å¯¾å¿œã®å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«å
        safe_chars = re.sub(r'[<>:"/\\|?*]', '_', filename)
        safe_chars = re.sub(r'\s+', '_', safe_chars)
        return safe_chars[:100]  # é•·ã•åˆ¶é™

# Utility functions (å¤‰æ›´ãªã—)
def get_path_part(n):
    relpath = os.path.dirname(n)
    if len(relpath) > 0:
        relpath = relpath + "/"
    return relpath

def get_file_part(n):
    return os.path.basename(n)

def main(argv):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•° v2.0"""
    from optparse import OptionParser
    
    usage = 'usage: python %prog [options] <input epub>'
    parser = OptionParser(usage + '''

Wordå½¢å¼ç›®æ¬¡ãƒ¬ãƒ™ãƒ«3æ®µéšå‡ºåŠ›ãƒ„ãƒ¼ãƒ« v2.0 - Calibreäº’æ›ç‰ˆ

EPUBãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®æ¬¡æ§‹é€ ã‚’åˆ†æã—ã€Wordå½¢å¼ã§3æ®µéšã¾ã§è©³ç´°ã«å‡ºåŠ›ã—ã¾ã™ã€‚
v2.0ã§ã¯ã€Calibreã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’æ¡ç”¨ã—ã€ãƒãƒƒãƒå‡¦ç†ã«ã‚‚å¯¾å¿œã€‚
''')

    parser.add_option("-o", "--output-dir", dest="output_dir", default=".",
                      help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)", metavar="DIR")
    parser.add_option("-f", "--format", dest="format", default="both",
                      help="å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®š: text, word, both (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: both)", metavar="FORMAT")
    parser.add_option("-b", "--batch", dest="batch_dir", default=None,
                      help="ãƒãƒƒãƒå‡¦ç†: æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨EPUBãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†", metavar="DIR")
    parser.add_option("--workers", dest="workers", type="int", default=4,
                      help="ãƒãƒƒãƒå‡¦ç†ã®ä¸¦åˆ—å®Ÿè¡Œæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4)", metavar="NUM")

    (options, args) = parser.parse_args(argv)

    # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    if options.batch_dir:
        print(f"ğŸ”„ ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {options.batch_dir}")
        processor = BatchProcessor(max_workers=options.workers)
        results = processor.process_directory(
            options.batch_dir, 
            output_dir=options.output_dir,
            format_type=options.format
        )
        
        # çµæœã‚µãƒãƒªãƒ¼
        success_count = len([r for r in results if r['status'] == 'success'])
        error_count = len([r for r in results if r['status'] == 'error'])
        
        print(f"\nğŸ“Š ãƒãƒƒãƒå‡¦ç†çµæœ:")
        print(f"âœ… æˆåŠŸ: {success_count}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        if processor.errors:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼è©³ç´°:")
            for error in processor.errors[:5]:  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
                print(f"  - {error}")
        
        return

    # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    if not args:
        parser.print_help()
        return

    epub_path = args[0]
    if not os.path.exists(epub_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {epub_path}")
        return

    print(f"ğŸ“š EPUBåˆ†æé–‹å§‹: {epub_path}")
    
    try:
        with open(epub_path, 'rb') as f:
            epub_splitter = SplitEpubWordTOC(f)
            results = epub_splitter.generate_word_toc_output(
                output_dir=options.output_dir,
                format_type=options.format
            )
        
        print("\nâœ… ç›®æ¬¡åˆ†æå®Œäº†!")
        print(f"ğŸ“Š å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        for format_type, path in results.items():
            print(f"   - {format_type}: {path}")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"Error processing EPUB: {e}", exc_info=True)

if __name__ == "__main__":
    main(sys.argv[1:])
