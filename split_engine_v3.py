#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
EPUBç›®æ¬¡è§£æãƒ„ãƒ¼ãƒ« v3.0 - åˆ†å‰²æ©Ÿèƒ½ã‚¨ãƒ³ã‚¸ãƒ³
å‹•çš„ç›®æ¬¡è§£æãƒ»Wordãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ãƒ»ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›å¯¾å¿œ
"""

__version__ = '3.0.0'
__license__ = 'GPL v3'
__copyright__ = '2025, Enhanced Split Engine v3.0'

import os
import re
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from collections import defaultdict

# Wordå‡¦ç†
try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.document import Document as DocxDocument
    from docx.text.paragraph import Paragraph
    import docx2txt
    WORD_SUPPORT = True
except ImportError:
    WORD_SUPPORT = False

# PDFç”Ÿæˆ
try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph as RLParagraph, Spacer, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

# EPUBç”Ÿæˆ
try:
    import ebooklib
    from ebooklib import epub
    EPUB_SUPPORT = True
except ImportError:
    EPUB_SUPPORT = False

# æ—¢å­˜v2.0ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from epubsplit_word_toc_v2 import SplitEpubWordTOC, CalibreCompatibleTOCDetector
    V2_SUPPORT = True
except ImportError:
    V2_SUPPORT = False

@dataclass
class TOCEntry:
    """ç›®æ¬¡ã‚¨ãƒ³ãƒˆãƒªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    text: str
    level: int
    page_num: Optional[int] = None
    paragraph_index: Optional[int] = None
    style_name: Optional[str] = None
    parent_path: str = ""
    file_href: str = ""
    anchor: str = ""
    
    def __post_init__(self):
        if self.parent_path and self.text:
            self.full_path = f"{self.parent_path}/{self.text}"
        else:
            self.full_path = self.text

@dataclass 
class SplitConfig:
    """åˆ†å‰²è¨­å®šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    split_level: int = 1  # åˆ†å‰²ã™ã‚‹ç›®æ¬¡ãƒ¬ãƒ™ãƒ«
    output_format: str = "word"  # word, pdf, epub
    include_subsections: bool = True  # ä¸‹ä½ãƒ¬ãƒ™ãƒ«ã‚‚å«ã‚ã‚‹
    preserve_formatting: bool = True  # æ›¸å¼ä¿æŒ
    output_dir: str = "output"
    filename_pattern: str = "{index:02d}_{title}"
    max_filename_length: int = 50

class DynamicTOCAnalyzer:
    """å‹•çš„ç›®æ¬¡æ§‹é€ è§£æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.toc_entries: List[TOCEntry] = []
        self.level_stats: Dict[int, int] = defaultdict(int)
        self.max_depth = 0
        self.recommended_split_level = 1
        
    def analyze_word_document(self, docx_path: str) -> Dict[str, Any]:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®æ¬¡æ§‹é€ ã‚’è§£æ"""
        if not WORD_SUPPORT:
            raise ImportError("python-docx is required for Word analysis")
            
        document = Document(docx_path)
        self.toc_entries = []
        self.level_stats = defaultdict(int)
        
        # è¦‹å‡ºã—ã‚¹ã‚¿ã‚¤ãƒ«ã®æ¤œå‡º
        heading_styles = self._detect_heading_styles(document)
        
        # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•è§£æ
        for para_idx, paragraph in enumerate(document.paragraphs):
            if paragraph.style.name in heading_styles:
                level = heading_styles[paragraph.style.name]
                text = paragraph.text.strip()
                
                if text:  # ç©ºã§ãªã„è¦‹å‡ºã—ã®ã¿
                    entry = TOCEntry(
                        text=text,
                        level=level,
                        paragraph_index=para_idx,
                        style_name=paragraph.style.name
                    )
                    self.toc_entries.append(entry)
                    self.level_stats[level] += 1
        
        # éšå±¤æ§‹é€ ã®æ§‹ç¯‰
        self._build_hierarchy()
        
        # åˆ†æçµæœã‚’ã¾ã¨ã‚ã‚‹
        return self._generate_analysis_report()
    
    def analyze_epub_toc(self, epub_path: str) -> Dict[str, Any]:
        """EPUBãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®æ¬¡æ§‹é€ ã‚’è§£æ"""
        if not V2_SUPPORT:
            raise ImportError("v2.0 modules required for EPUB analysis")
            
        with open(epub_path, 'rb') as f:
            epub_analyzer = SplitEpubWordTOC(f)
            toc_map = epub_analyzer.get_enhanced_toc_map()
            
            # EPUBç›®æ¬¡ã‚’TOCEntryã«å¤‰æ›
            self.toc_entries = []
            self.level_stats = defaultdict(int)
            
            if epub_analyzer.toc_processor:
                for level in range(1, 4):
                    for entry in epub_analyzer.toc_processor.word_toc_generator.levels[level]:
                        toc_entry = TOCEntry(
                            text=entry['text'],
                            level=level,
                            file_href=entry.get('href', ''),
                            anchor=entry.get('anchor', ''),
                            parent_path=entry.get('hierarchy_path', '')
                        )
                        self.toc_entries.append(toc_entry)
                        self.level_stats[level] += 1
        
        return self._generate_analysis_report()
    
    def _detect_heading_styles(self, document: DocxDocument) -> Dict[str, int]:
        """è¦‹å‡ºã—ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¦ãƒ¬ãƒ™ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°"""
        heading_styles = {}
        
        # æ¨™æº–çš„ãªè¦‹å‡ºã—ã‚¹ã‚¿ã‚¤ãƒ«
        standard_headings = {
            'Heading 1': 1, 'Heading 2': 2, 'Heading 3': 3,
            'Heading 4': 4, 'Heading 5': 5, 'Heading 6': 6,
            'è¦‹å‡ºã— 1': 1, 'è¦‹å‡ºã— 2': 2, 'è¦‹å‡ºã— 3': 3,
            'Title': 1, 'Subtitle': 2
        }
        
        # ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ç¢ºèª
        used_styles = set()
        for paragraph in document.paragraphs:
            if paragraph.text.strip():
                used_styles.add(paragraph.style.name)
        
        # æ¨™æº–ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        for style_name, level in standard_headings.items():
            if style_name in used_styles:
                heading_styles[style_name] = level
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«ã®æ¤œå‡ºï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
        for style_name in used_styles:
            if style_name not in heading_styles:
                # ã‚¹ã‚¿ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
                if re.search(r'(chapter|ç« |section|ç¯€)', style_name, re.IGNORECASE):
                    heading_styles[style_name] = 1
                elif re.search(r'(subsection|å°ç¯€|éƒ¨)', style_name, re.IGNORECASE):
                    heading_styles[style_name] = 2
        
        return heading_styles
    
    def _build_hierarchy(self):
        """éšå±¤æ§‹é€ ã‚’æ§‹ç¯‰"""
        if not self.toc_entries:
            return
            
        # è¦ªå­é–¢ä¿‚ã®æ§‹ç¯‰
        stack = []  # (level, entry) ã®ã‚¹ã‚¿ãƒƒã‚¯
        
        for entry in self.toc_entries:
            # ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®è¦ç´ ã‚’ã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰é™¤å»
            while stack and stack[-1][0] >= entry.level:
                stack.pop()
            
            # è¦ªãƒ‘ã‚¹ã®æ§‹ç¯‰
            if stack:
                parent_entry = stack[-1][1]
                entry.parent_path = parent_entry.full_path
                entry.full_path = f"{entry.parent_path}/{entry.text}"
            
            stack.append((entry.level, entry))
        
        # çµ±è¨ˆæ›´æ–°
        self.max_depth = max(self.level_stats.keys()) if self.level_stats else 0
        
        # æ¨å¥¨åˆ†å‰²ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š
        self._calculate_recommended_split_level()
    
    def _calculate_recommended_split_level(self):
        """æ¨å¥¨åˆ†å‰²ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        if not self.level_stats:
            self.recommended_split_level = 1
            return
        
        # ãƒ¬ãƒ™ãƒ«1ã®æ•°ãŒé©åº¦ï¼ˆ3-20ç« ï¼‰ãªã‚‰1ã§åˆ†å‰²
        level1_count = self.level_stats.get(1, 0)
        if 3 <= level1_count <= 20:
            self.recommended_split_level = 1
        # ãƒ¬ãƒ™ãƒ«1ãŒå°‘ãªãã¦ãƒ¬ãƒ™ãƒ«2ãŒå¤šã„ãªã‚‰2ã§åˆ†å‰²
        elif level1_count < 3 and self.level_stats.get(2, 0) > 5:
            self.recommended_split_level = 2
        # ãƒ¬ãƒ™ãƒ«1ãŒå¤šã™ãã‚‹ãªã‚‰2ã§åˆ†å‰²ã‚’ææ¡ˆ
        elif level1_count > 20:
            self.recommended_split_level = 2
        else:
            self.recommended_split_level = 1
    
    def _generate_analysis_report(self) -> Dict[str, Any]:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = {
            'total_entries': len(self.toc_entries),
            'max_depth': self.max_depth,
            'level_stats': dict(self.level_stats),
            'recommended_split_level': self.recommended_split_level,
            'entries': [
                {
                    'text': entry.text,
                    'level': entry.level,
                    'full_path': entry.full_path
                } for entry in self.toc_entries
            ],
            'split_preview': self._generate_split_preview()
        }
        return report
    
    def _generate_split_preview(self) -> List[Dict]:
        """åˆ†å‰²ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆ"""
        preview = []
        split_level = self.recommended_split_level
        
        current_section = None
        subsections = []
        
        for entry in self.toc_entries:
            if entry.level == split_level:
                # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                if current_section:
                    preview.append({
                        'title': current_section.text,
                        'subsections': len(subsections),
                        'subsection_list': [s.text for s in subsections[:5]]  # æœ€åˆã®5å€‹ã®ã¿
                    })
                
                # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
                current_section = entry
                subsections = []
            elif entry.level > split_level and current_section:
                subsections.append(entry)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if current_section:
            preview.append({
                'title': current_section.text,
                'subsections': len(subsections),
                'subsection_list': [s.text for s in subsections[:5]]
            })
        
        return preview

class WordDocumentSplitter:
    """Wordãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, analyzer: DynamicTOCAnalyzer):
        self.analyzer = analyzer
        self.original_document = None
        self.split_documents = []
        
    def split_document(self, docx_path: str, config: SplitConfig) -> List[str]:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›®æ¬¡ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ã„ã¦åˆ†å‰²"""
        if not WORD_SUPPORT:
            raise ImportError("python-docx is required for Word splitting")
        
        self.original_document = Document(docx_path)
        output_files = []
        
        # åˆ†å‰²ãƒã‚¤ãƒ³ãƒˆã®æ±ºå®š
        split_points = self._determine_split_points(config.split_level)
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²
        for i, (start_idx, end_idx, title) in enumerate(split_points):
            section_doc = self._create_section_document(start_idx, end_idx, title, config)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            safe_title = self._sanitize_filename(title)
            filename = config.filename_pattern.format(
                index=i+1,
                title=safe_title[:config.max_filename_length]
            )
            
            output_path = Path(config.output_dir) / f"{filename}.docx"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            section_doc.save(str(output_path))
            output_files.append(str(output_path))
        
        return output_files
    
    def _determine_split_points(self, split_level: int) -> List[Tuple[int, int, str]]:
        """åˆ†å‰²ãƒã‚¤ãƒ³ãƒˆã‚’æ±ºå®š"""
        split_points = []
        current_start = 0
        
        for i, entry in enumerate(self.analyzer.toc_entries):
            if entry.level == split_level:
                # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†
                if i > 0:  # æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã§ãªã„å ´åˆ
                    prev_end = entry.paragraph_index
                    split_points.append((current_start, prev_end, prev_title))
                
                # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
                current_start = entry.paragraph_index
                prev_title = entry.text
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if self.analyzer.toc_entries:
            split_points.append((current_start, len(self.original_document.paragraphs), prev_title))
        
        return split_points
    
    def _create_section_document(self, start_idx: int, end_idx: int, title: str, config: SplitConfig) -> DocxDocument:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ–‡æ›¸ã‚’ä½œæˆ"""
        section_doc = Document()
        
        # ã‚¿ã‚¤ãƒˆãƒ«è¿½åŠ 
        title_para = section_doc.add_heading(title, level=1)
        
        # å…ƒæ–‡æ›¸ã®æ®µè½ã‚’ã‚³ãƒ”ãƒ¼
        for i in range(start_idx, min(end_idx, len(self.original_document.paragraphs))):
            original_para = self.original_document.paragraphs[i]
            
            if config.preserve_formatting:
                # æ›¸å¼ã‚’ä¿æŒã—ã¦ã‚³ãƒ”ãƒ¼
                new_para = section_doc.add_paragraph()
                new_para.style = original_para.style
                
                for run in original_para.runs:
                    new_run = new_para.add_run(run.text)
                    new_run.bold = run.bold
                    new_run.italic = run.italic
                    new_run.underline = run.underline
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚³ãƒ”ãƒ¼
                section_doc.add_paragraph(original_para.text)
        
        return section_doc
    
    def _sanitize_filename(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã®ç„¡åŠ¹æ–‡å­—ã‚’é™¤å»"""
        # ç„¡åŠ¹æ–‡å­—ã‚’é™¤å»
        safe_name = re.sub(r'[<>:"/\\|?*]', '_', filename)
        safe_name = re.sub(r'\s+', '_', safe_name)
        safe_name = safe_name.strip('._')
        
        # ç©ºã®å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå
        if not safe_name:
            safe_name = "section"
        
        return safe_name

class MultiFormatExporter:
    """è¤‡æ•°å½¢å¼å‡ºåŠ›ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, analyzer: DynamicTOCAnalyzer):
        self.analyzer = analyzer
    
    def export_to_pdf(self, content_sections: List[Dict], output_path: str) -> str:
        """PDFå½¢å¼ã§å‡ºåŠ›"""
        if not PDF_SUPPORT:
            raise ImportError("reportlab is required for PDF export")
        
        doc = SimpleDocTemplate(output_path, pagesize=A4)
        styles = getSampleStyleSheet()
        story = []
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        try:
            # Windowsç’°å¢ƒã§ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
            font_paths = [
                "C:/Windows/Fonts/msgothic.ttc",  # MS ã‚´ã‚·ãƒƒã‚¯
                "C:/Windows/Fonts/meiryo.ttc",    # ãƒ¡ã‚¤ãƒªã‚ª
            ]
            for font_path in font_paths:
                if os.path.exists(font_path):
                    pdfmetrics.registerFont(TTFont('Japanese', font_path))
                    break
        except:
            pass  # ãƒ•ã‚©ãƒ³ãƒˆç™»éŒ²ã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
        
        for section in content_sections:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
            title = RLParagraph(section['title'], styles['Title'])
            story.append(title)
            story.append(Spacer(1, 12))
            
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…å®¹
            for paragraph_text in section.get('content', []):
                para = RLParagraph(paragraph_text, styles['Normal'])
                story.append(para)
                story.append(Spacer(1, 6))
            
            story.append(PageBreak())
        
        doc.build(story)
        return output_path
    
    def export_to_epub(self, content_sections: List[Dict], output_path: str, metadata: Dict = None) -> str:
        """EPUBå½¢å¼ã§å‡ºåŠ›"""
        if not EPUB_SUPPORT:
            raise ImportError("ebooklib is required for EPUB export")
        
        book = epub.EpubBook()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
        metadata = metadata or {}
        book.set_identifier(f"epub_split_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        book.set_title(metadata.get('title', 'Split Document'))
        book.set_language(metadata.get('language', 'ja'))
        book.add_author(metadata.get('author', 'Unknown'))
        
        # ç« ã‚’è¿½åŠ 
        spine = ['nav']
        toc = []
        
        for i, section in enumerate(content_sections):
            # ç« ã®ä½œæˆ
            chapter_filename = f"chapter_{i+1}.xhtml"
            chapter = epub.EpubHtml(
                title=section['title'],
                file_name=chapter_filename,
                lang='ja'
            )
            
            # HTMLå†…å®¹ç”Ÿæˆ
            content_html = f"<h1>{section['title']}</h1>"
            for paragraph_text in section.get('content', []):
                content_html += f"<p>{paragraph_text}</p>"
            
            chapter.set_content(content_html)
            book.add_item(chapter)
            spine.append(chapter)
            toc.append(epub.Link(chapter_filename, section['title'], f"chapter_{i+1}"))
        
        # ç›®æ¬¡è¨­å®š
        book.toc = toc
        book.spine = spine
        
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ 
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        
        # EPUBæ›¸ãå‡ºã—
        epub.write_epub(output_path, book)
        return output_path

def main():
    """v3.0ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ“š EPUBç›®æ¬¡è§£æãƒ„ãƒ¼ãƒ« v3.0 - åˆ†å‰²æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # å‹•çš„TOCè§£æã®ãƒ†ã‚¹ãƒˆ
    analyzer = DynamicTOCAnalyzer()
    
    # ä½¿ç”¨ä¾‹ã®è¡¨ç¤º
    print("ğŸ’¡ ä½¿ç”¨ä¾‹:")
    print("1. Wordãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ:")
    print("   analyzer = DynamicTOCAnalyzer()")
    print("   report = analyzer.analyze_word_document('document.docx')")
    print("")
    print("2. åˆ†å‰²å®Ÿè¡Œ:")
    print("   splitter = WordDocumentSplitter(analyzer)")
    print("   config = SplitConfig(split_level=1, output_format='word')")
    print("   files = splitter.split_document('document.docx', config)")
    print("")
    print("3. è¤‡æ•°å½¢å¼å‡ºåŠ›:")
    print("   exporter = MultiFormatExporter(analyzer)")
    print("   exporter.export_to_pdf(sections, 'output.pdf')")
    print("   exporter.export_to_epub(sections, 'output.epub')")
    
    return True

if __name__ == "__main__":
    main()
